# AutoViz Agent Configuration

# Default model to use
default_model: "qwen-1.5b-vllm"  # Change from "phi-3.5-mini"

# Model definitions
models:
  phi-4-mini:
    name: "Phi-4-mini-instruct"
    backend: "llama.cpp"
    path: "models/Phi-4-mini-instruct-Q4_K_M.gguf"
    context_length: 16384
    quantization: "Q4_K_M"
    n_gpu_layers: -1  # Use all available GPU layers
    temperature: 0.1  # Low for deterministic behavior
    top_p: 0.9
    max_tokens: 4096
    
  phi-3.5-mini:
    name: "Phi-3.5-mini-instruct"
    backend: "llama.cpp"
    path: "models/Phi-3.5-mini-instruct-Q4_K_M.gguf"
    context_length: 4096
    quantization: "Q4_K_M"
    n_gpu_layers: -1  # Use all available GPU layers
    temperature: 0.1  # Low for deterministic behavior
    top_p: 0.9
    max_tokens: 2048
    
  llama-3.1-8b:
    name: "Llama 3.1 8B Instruct"
    backend: "llama.cpp"
    path: "models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"
    context_length: 8192
    quantization: "Q4_K_M"
    n_gpu_layers: -1
    temperature: 0.1
    top_p: 0.9
    max_tokens: 2048
    
  mistral-7b:
    name: "Mistral 7B Instruct v0.3"
    backend: "llama.cpp"
    path: "models/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf"
    context_length: 8192
    quantization: "Q4_K_M"
    n_gpu_layers: -1
    temperature: 0.1
    top_p: 0.9
    max_tokens: 2048
    
  gemma-2-9b:
    name: "Gemma 2 9B Instruct"
    backend: "llama.cpp"
    path: "models/gemma-2-9b-it-Q4_K_M.gguf"
    context_length: 8192
    quantization: "Q4_K_M"
    n_gpu_layers: -1
    temperature: 0.1
    top_p: 0.9
    max_tokens: 2048

  # vLLM backend models (run in WSL2 with GPU acceleration)
  phi-4-mini-vllm:
    name: "Phi-4-mini-instruct"
    backend: "vllm"
    url: "http://localhost:8000"
    model_name: "phi-4-mini"  # Model identifier on vLLM server
    temperature: 0.1
    max_tokens: 2048
    use_grammar: true  # Enable xgrammar2 for structured outputs

  llama-3.1-8b-vllm:
    name: "Llama 3.1 8B Instruct"
    backend: "vllm"
    url: "http://localhost:8000"
    model_name: "llama-3.1-8b"
    temperature: 0.1
    max_tokens: 2048
    use_grammar: true

  qwen-1.5b-vllm:
    name: "Qwen 2.5 1.5B Instruct"
    backend: "vllm"
    url: "http://localhost:8000"
    model_name: "/home/effguilar/models/qwen-1.5b-4bit"  # Model identifier from server
    temperature: 0.1
    max_tokens: 1024  # Model's max context
    use_grammar: true  # Enable xgrammar2 for structured outputs

# Backend-specific settings
backends:
  llama.cpp:
    verbose: false
    seed: 42  # For deterministic generation
    n_batch: 512
    n_threads: 8
    
  vllm:
    # vLLM server configuration (runs in WSL2 with GPU)
    url: "http://localhost:8000"
    timeout: 60  # Request timeout in seconds
    use_grammar: true  # Enable xgrammar2 for structured JSON outputs
    verify_ssl: false  # Set to true if using HTTPS with valid cert

# Plan templates directory
templates_dir: "templates"

# Output settings
output:
  base_dir: "outputs"
  artifacts:
    - "plan_template.json"
    - "plan_adapted.json"
    - "plan_diff.md"
    - "tool_calls.json"
    - "execution_log.json"
    - "report.md"
  charts_dir: "charts"

# Intent classification settings
intent:
  max_intents: 3  # Top-N intents to consider
  labels:
    - "general_eda"
    - "time_series_investigation"
    - "segmentation_drivers"
    - "anomaly_detection"
    - "comparative_analysis"

# Logging
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
