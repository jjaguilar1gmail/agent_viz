#!/usr/bin/env python
"""Test script to verify LLM factory and backend selection."""

import sys
import yaml
from pathlib import Path

sys.path.insert(0, 'src')

from autoviz_agent.llm.factory import create_llm_client

def main():
    print("=" * 60)
    print("Testing LLM Factory and Backend Selection")
    print("=" * 60)
    
    # Load configuration (same way as nodes.py)
    config_path = Path("config.yaml")
    if not config_path.exists():
        print("‚ùå config.yaml not found!")
        return
        
    with open(config_path) as f:
        config = yaml.safe_load(f)
    
    default_model = config['default_model']
    model_config = config['models'][default_model]
    
    print(f"\n‚úì Loaded configuration")
    print(f"  Default model: {default_model}")
    print(f"  Backend: {model_config.get('backend', 'llama.cpp')}")
    
    # Create client using factory
    client = create_llm_client(model_config)
    
    print(f"\n‚úì Created client instance")
    print(f"  Class: {client.__class__.__name__}")
    print(f"  Module: {client.__class__.__module__}")
    
    # Test prompt building with minimal schema object
    from autoviz_agent.llm.prompts import PromptBuilder
    from autoviz_agent.models.state import SchemaProfile, ColumnProfile
    prompt_builder = PromptBuilder()
    
    # Create a test schema
    test_schema = SchemaProfile(
        columns=[
            ColumnProfile(name="revenue", dtype="float64", missing_rate=0.0, cardinality=100),
            ColumnProfile(name="date", dtype="datetime64", missing_rate=0.0, cardinality=30),
            ColumnProfile(name="region", dtype="object", missing_rate=0.0, cardinality=5)
        ],
        row_count=100,
        data_shape="tabular"
    )
    test_query = "Analyze revenue trends by region"
    
    intent_prompt = prompt_builder.build_intent_prompt(test_query, test_schema)
    
    print(f"\n‚úì Built test prompt")
    print(f"  Prompt length: {len(intent_prompt)} characters")
    print(f"  Contains system: {'<|system|>' in intent_prompt}")
    print(f"  Contains user: {'<|user|>' in intent_prompt}")
    
    print("\n" + "=" * 60)
    print("‚úÖ All tests passed! System is ready.")
    print("=" * 60)
    
    print("\nüìã Summary:")
    print(f"  - LLM Factory: Working")
    print(f"  - Backend: {model_config.get('backend', 'llama.cpp')}")
    print(f"  - Prompt System: Working")
    print(f"  - Ready for execution: Yes")
    
    print("\n‚ö†Ô∏è  Note: vLLM server requires PyTorch fix (see docs/VLLM_STATUS.md)")
    print("    Currently using gpt4all backend which is fully functional.")

if __name__ == "__main__":
    main()
